# agent.py
import json
import logging
import time
import re
from playwright.sync_api import Error as PlaywrightError, TimeoutError as PlaywrightTimeoutError
from typing import Dict, Any, Optional, List, Tuple
import random
import os
import threading # For timer
from datetime import datetime

# Use relative imports within the package
from browser_controller import BrowserController
from llm_client import GeminiClient
from vision_processor import VisionProcessor
from task_manager import TaskManager
from dom.views import DOMState, DOMElementNode, SelectorMap # Import DOM types

# Configure logger
logger = logging.getLogger(__name__)

# --- Recorder Settings ---
INTERACTIVE_TIMEOUT_SECS = 5 # Time for user to override AI suggestion
DEFAULT_WAIT_AFTER_ACTION = 0.5 # Default small wait added after recorded actions
# --- End Recorder Settings ---


class WebAgent:
    """
    Orchestrates AI-assisted web test recording, generating reproducible test scripts.
    Can also function in a (now legacy) execution mode.
    """

    def __init__(self,
                 gemini_client: GeminiClient,
                 headless: bool = True, # Note: Recorder mode forces non-headless
                 max_iterations: int = 50, # Max planned steps to process in recorder
                 max_history_length: int = 10,
                 max_retries_per_subtask: int = 1, # Retries for *AI suggestion* or failed *execution* during recording
                 max_extracted_data_history: int = 7, # Less relevant for recorder? Keep for now.
                 is_recorder_mode: bool = False): # <<< New mode flag

        self.gemini_client = gemini_client
        # Recorder mode MUST run with a visible browser window
        self.is_recorder_mode = is_recorder_mode
        effective_headless = False if self.is_recorder_mode else headless
        if self.is_recorder_mode and headless:
             logger.warning("Recorder mode initiated, but headless=True was requested. Forcing headless=False for interaction.")

        self.browser_controller = BrowserController(headless=effective_headless)
        self.vision_processor = VisionProcessor(gemini_client)
        # TaskManager manages the *planned* steps generated by LLM initially
        self.task_manager = TaskManager(max_retries_per_subtask=max_retries_per_subtask)
        self.history: List[Dict[str, Any]] = []
        self.extracted_data_history: List[Dict[str, Any]] = [] # Keep for potential context, but less critical now
        self.max_iterations = max_iterations # Limit for planned steps processing
        self.max_history_length = max_history_length
        self.max_extracted_data_history = max_extracted_data_history
        self.output_file_path: Optional[str] = None # Path for the recorded JSON
        self.feature_description: Optional[str] = None
        self._latest_dom_state: Optional[DOMState] = None
        # --- Recorder Specific State ---
        self.recorded_steps: List[Dict[str, Any]] = []
        self._current_step_id = 1 # Counter for recorded steps
        self._user_abort_recording = False
        # --- End Recorder Specific State ---

        # Log effective mode
        mode_name = "Recorder" if self.is_recorder_mode else "Execution (Legacy)"
        logger.info(f"WebAgent ({mode_name} Mode) initialized (headless={effective_headless}, max_planned_steps={max_iterations}, max_hist={max_history_length}, max_retries={max_retries_per_subtask}).")


    def _add_to_history(self, entry_type: str, data: Any):
        """Adds an entry to the agent's history, maintaining max length."""
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        log_data_str = "..."
        try:
            # Basic sanitization (same as before)
            if isinstance(data, dict):
                log_data = {k: (str(v)[:200] + '...' if len(str(v)) > 200 else v)
                             for k, v in data.items()}
            elif isinstance(data, (str, bytes)):
                 log_data = str(data[:297]) + "..." if len(data) > 300 else str(data)
            else:
                 log_data = data
            log_data_str = str(log_data)
            if len(log_data_str) > 300: log_data_str = log_data_str[:297]+"..."
        except Exception as e:
            logger.warning(f"Error sanitizing history data: {e}")
            log_data = f"Error processing data: {e}"
            log_data_str = log_data

        entry = {"timestamp": timestamp, "type": entry_type, "data": log_data}
        self.history.append(entry)
        if len(self.history) > self.max_history_length:
            self.history.pop(0)
        logger.debug(f"[HISTORY] Add: {entry_type} - {log_data_str}")

    def _get_history_summary(self) -> str:
        """Provides a concise summary of the recent history for the LLM."""
        # (Implementation remains the same as before)
        if not self.history: return "No history yet."
        summary = "Recent History (Oldest First):\n"
        for entry in self.history:
            entry_data_str = str(entry['data'])
            if len(entry_data_str) > 300: entry_data_str = entry_data_str[:297] + "..."
            summary += f"- [{entry['type']}] {entry_data_str}\n"
        return summary.strip()

    def _clean_llm_response_to_json(self, llm_output: str) -> Optional[Dict[str, Any]]:
        """Attempts to extract and parse JSON from the LLM's output."""
        # (Implementation remains the same as before)
        logger.debug(f"[LLM PARSE] Attempting to parse LLM response (length: {len(llm_output)}).")
        match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", llm_output, re.DOTALL | re.IGNORECASE)
        if match:
            json_str = match.group(1).strip()
            logger.debug(f"[LLM PARSE] Extracted JSON from markdown block.")
        else:
            start_index = llm_output.find('{')
            end_index = llm_output.rfind('}')
            if start_index != -1 and end_index != -1 and end_index > start_index:
                json_str = llm_output[start_index : end_index + 1].strip()
                logger.debug(f"[LLM PARSE] Attempting to parse extracted JSON between {{ and }}.")
            else:
                 logger.warning("[LLM PARSE] Could not find JSON structure in LLM output.")
                 self._add_to_history("LLM Parse Error", {"reason": "No JSON structure found", "raw_output_snippet": llm_output[:200]})
                 return None

        # Pre-processing (same as before)
        try:
            def escape_quotes_replacer(match):
                key_part, colon_part, open_quote, value, close_quote = match.groups()
                escaped_value = re.sub(r'(?<!\\)"', r'\\"', value)
                return f'{key_part}{colon_part}{open_quote}{escaped_value}{close_quote}'
            keys_to_escape = ["selector", "text", "reasoning", "url", "result", "answer", "reason", "file_path", "expected_text", "attribute_name", "expected_value"]
            pattern_str = r'(\"(?:' + '|'.join(keys_to_escape) + r')\")(\s*:\s*)(\")(.*?)(\")'
            pattern = re.compile(pattern_str, re.DOTALL)
            json_str = pattern.sub(escape_quotes_replacer, json_str)
            json_str = json_str.replace('\\\\n', '\\n').replace('\\n', '\n')
            json_str = json_str.replace('\\\\"', '\\"')
            json_str = json_str.replace('\\\\t', '\\t')
            json_str = re.sub(r',\s*([\}\]])', r'\1', json_str)
        except Exception as clean_e:
             logger.warning(f"[LLM PARSE] Error during pre-parsing cleaning: {clean_e}")

        # Attempt Parsing (check for 'action' primarily, parameters might be optional for some recorder actions)
        try:
            parsed_json = json.loads(json_str)
            if isinstance(parsed_json, dict) and "action" in parsed_json:
                # Parameters might not always be present (e.g., simple scroll)
                if "parameters" not in parsed_json:
                     parsed_json["parameters"] = {} # Ensure parameters key exists
                logger.debug(f"[LLM PARSE] Successfully parsed action JSON: {parsed_json}")
                return parsed_json
            else:
                 logger.warning(f"[LLM PARSE] Parsed JSON missing 'action' key or is not a dict: {parsed_json}")
                 self._add_to_history("LLM Parse Error", {"reason": "Missing 'action' key", "parsed_json": parsed_json, "cleaned_json_string": json_str[:200]})
                 return None
        except json.JSONDecodeError as e:
            logger.error(f"[LLM PARSE] Failed to decode JSON from LLM output: {e}")
            logger.error(f"[LLM PARSE] Faulty JSON string snippet (around pos {e.pos}): {json_str[max(0, e.pos-50):e.pos+50]}")
            self._add_to_history("LLM Parse Error", {"reason": f"JSONDecodeError: {e}", "error_pos": e.pos, "json_string_snippet": json_str[max(0, e.pos-50):e.pos+50]})
            return None
        except Exception as e:
             logger.error(f"[LLM PARSE] Unexpected error during final JSON parsing: {e}", exc_info=True)
             return None

    def _plan_subtasks(self, feature_description: str):
        """Uses the LLM to break down the feature test into planned steps (remains largely the same)."""
        logger.info(f"Planning test steps for feature: '{feature_description}'")
        self.feature_description = feature_description

        # --- Prompt Construction (Keep similar focus on actions and verifications) ---
        prompt = f"""
        You are an AI Test Engineer planning steps for recording. Given the feature description: "{feature_description}"

        Break this down into a sequence of specific browser actions or verification checks.
        Each step should be a single instruction (e.g., "Navigate to...", "Click the 'Submit' button", "Type 'testuser' into username field", "Verify text 'Success' is visible").
        The recorder agent will handle identifying elements and generating selectors based on these descriptions.

        **Key Types of Steps to Plan:**
        1.  **Navigation:** `Navigate to https://example.com/login`
        2.  **Action:** `Click element 'Submit Button'` or `Type 'testuser' into element 'Username Input'` (Describe the element clearly)
        3.  **Verification:** Phrase as a check. The recorder will prompt for specifics.
            - `Verify 'Login Successful' message is present`
            - `Verify 'Cart Count' shows 1`
            - `Verify 'Submit' button is disabled`
        4.  **Scrolling:** `Scroll down` (if content might be off-screen)

        **CRITICAL:** Focus on the *intent* of each step. Do NOT include specific selectors or indices in the plan. The recorder determines those interactively.

        **Output ONLY the test steps as a JSON list of strings.** No explanations or markdown.

        Example Test Case: "Test login on example.com with user 'tester' and pass 'pwd123', then verify the welcome message 'Welcome, tester!' is shown."
        Example JSON Output:
        ```json
        [
          "Navigate to https://example.com/login",
          "Type 'tester' into element 'username input field'",
          "Type 'pwd123' into element 'password input field'",
          "Click element 'login button'",
          "Verify 'Welcome, tester!' message is present"
        ]
        ```

        Now, generate the JSON list of planned steps for: "{feature_description}"
        JSON Test Step List:
        ```json
        """
        # --- End Prompt ---

        logger.debug(f"[TEST PLAN] Sending Planning Prompt (snippet):\n{prompt[:500]}...")
        response = self.gemini_client.generate_text(prompt)
        logger.debug(f"[TEST PLAN] LLM RAW response (snippet):\n{response[:500]}...")

        # --- Response Parsing (Same as before) ---
        subtasks = None
        json_str = None
        try:
             match = re.search(r"```(?:json)?\s*(\[.*?\])\s*```", response, re.DOTALL | re.IGNORECASE)
             if match: json_str = match.group(1).strip()
             else:
                  stripped_response = response.strip()
                  if stripped_response.startswith('[') and stripped_response.endswith(']'): json_str = stripped_response
                  else: logger.warning("[TEST PLAN] Could not find JSON list in markdown or as full response.")

             if json_str:
                  json_str_fixed = re.sub(r',\s*\]', ']', json_str)
                  json_str_fixed = re.sub(r',\s*\}', '}', json_str_fixed) # Just in case
                  parsed_list = json.loads(json_str_fixed)
                  if isinstance(parsed_list, list) and all(isinstance(s, str) and s for s in parsed_list):
                      subtasks = parsed_list
                  else: logger.warning(f"[TEST PLAN] Parsed JSON is not a list of non-empty strings: {parsed_list}")

        except json.JSONDecodeError as e:
             logger.error(f"[TEST PLAN] Failed to decode JSON subtask list: {e}")
             logger.debug(f"[TEST PLAN] Faulty JSON string for planning: {json_str}")
        except Exception as e:
            logger.error(f"[TEST PLAN] An unexpected error occurred during subtask parsing: {e}", exc_info=True)

        # --- Update Task Manager ---
        if subtasks and len(subtasks) > 0:
            self.task_manager.add_subtasks(subtasks) # TaskManager stores the *planned* steps
            self._add_to_history("Test Plan Created", {"feature": feature_description, "steps": subtasks})
            logger.info(f"Successfully planned {len(subtasks)} test steps.")
            logger.debug(f"[TEST PLAN] Planned Steps: {subtasks}")
        else:
            logger.error("[TEST PLAN] Failed to generate or parse valid planned steps from LLM response.")
            self._add_to_history("Test Plan Failed", {"feature": feature_description, "raw_response": response[:500]+"..."})
            raise ValueError("Failed to generate a valid test plan from the feature description.")

    def _get_extracted_data_summary(self) -> str:
        """Provides summary of extracted data (less critical for recorder, but kept for potential context)."""
        # (Implementation remains the same)
        if not self.extracted_data_history: return "No data extracted yet."
        summary = "Recently Extracted Data (Context Only):\n"
        start_index = max(0, len(self.extracted_data_history) - self.max_extracted_data_history)
        for entry in reversed(self.extracted_data_history[start_index:]):
             data_snippet = str(entry.get('data', ''))[:150] + "..." if len(str(entry.get('data', ''))) > 150 else str(entry.get('data', ''))
             step_desc_snippet = entry.get('subtask_desc', 'N/A')[:50] + ('...' if len(entry.get('subtask_desc', 'N/A')) > 50 else '')
             index_info = f"Index:[{entry.get('index', '?')}]"
             selector_info = f" Sel:'{entry.get('selector', '')[:30]}...'" if entry.get('selector') else ""
             summary += f"- Step {entry.get('subtask_index', '?')+1} ('{step_desc_snippet}'): {index_info}{selector_info} Type={entry.get('type')}, Data={data_snippet}\n"
        return summary.strip()


    def _determine_action_and_selector_for_recording(self,
                               current_task: Dict[str, Any],
                               current_url: str,
                               dom_context_str: str # Now contains indexed elements with PRE-GENERATED selectors
                               ) -> Optional[Dict[str, Any]]:
        """
        Uses LLM to propose the browser action (click, type) and identify the target *element index*
        based on the planned step description and the DOM context. The robust selector is retrieved
        from the DOM state afterwards.
        """
        logger.info(f"Determining AI suggestion for planned step: '{current_task['description']}'")

        # --- Modified Prompt for Recorder ---
        prompt = f"""
You are an AI assistant helping a user record a web test. Your goal is to interpret the user's planned step and identify the **single target interactive element** in the provided context that corresponds to it.

**Feature Under Test:** {self.feature_description}
**Current Planned Step:** {current_task['description']}
**Current URL:** {current_url}
**Test Recording Progress:** Attempt {current_task['attempts']} of {self.task_manager.max_retries_per_subtask + 1} for this suggestion.

**Input Context (Visible Interactive Elements with Indices):**
This section shows visible interactive elements on the page, each marked with `[index]` and its pre-generated robust CSS selector.
```html
{dom_context_str}
```

**Your Task:**
Based ONLY on the "Current Planned Step" description and the "Input Context":
1.  Identify the **single most likely interactive element `[index]`** from the context that matches the description in the planned step.
2.  Determine the appropriate **action** (`click` or `type`).
3.  If the action is `type`, extract the **text** to be typed from the planned step description.
4.  Provide brief **reasoning** linking the step description to the chosen index.

**Available Actions (Output JSON Format):**
-   `click`: Target element identified by index.
    ```json
    {{"action": "click", "parameters": {{"index": INDEX_NUMBER_FROM_CONTEXT}}, "reasoning": "The step asks to click the 'Login' button, which corresponds to element [12]."}}
    ```
-   `type`: Target element identified by index, include text.
    ```json
    {{"action": "type", "parameters": {{"index": INDEX_NUMBER_FROM_CONTEXT, "text": "extracted text"}}, "reasoning": "The step asks to type 'user@example.com' into the email field, which is element [5]."}}
    ```
-   `action_not_applicable`: Use if the planned step is navigation, verification, scrolling, or cannot be mapped to a click/type on a single element in the context.
    ```json
    {{"action": "action_not_applicable", "parameters": {{}}, "reasoning": "The step 'Navigate to ...' does not involve clicking or typing on an element from the context."}}
    ```
    ```json
    {{"action": "action_not_applicable", "parameters": {{}}, "reasoning": "The step 'Verify ...' is handled separately as an assertion."}}
    ```
-   `suggestion_failed`: Use if you cannot confidently identify a single matching element for the action described.
    ```json
    {{"action": "suggestion_failed", "parameters": {{}}, "reasoning": "Could not find a unique element matching 'the second confirmation button'."}}
    ```

**CRITICAL INSTRUCTIONS:**
-   **Focus on the `[index]`:** Your primary goal is to output the correct index for the planned step.
-   **Do NOT output selectors:** The recorder will get the selector from the context map using the index you provide.
-   **Handle non-interactive steps:** Use `action_not_applicable` for navigation, verification, scroll steps.
-   **Be precise:** If the step says "Type 'abc'", the text parameter must be exactly "abc". Extract text carefully.

**Output your decision strictly as a JSON object.**
```json
"""
        # --- End Prompt ---

        # Add error context if retrying suggestion
        if current_task['status'] == 'in_progress' and current_task['attempts'] > 1 and current_task.get('error'):
            error_context = str(current_task['error'])[:300] + "..."
            prompt += f"\n**Previous Suggestion Attempt Error:**\nAttempt {current_task['attempts'] - 1} failed: {error_context}\nRe-evaluate the description and context carefully.\n"

        # Add history summary for general context
        prompt += f"\n**Recent History (Context):**\n{self._get_history_summary()}\n"
        prompt += "**Your JSON Decision:**\n```json\n"


        logger.debug(f"[LLM RECORDER PROMPT] Sending prompt snippet for action/index suggestion:\n{prompt[:500]}...")

        response = self.gemini_client.generate_text(prompt)
        logger.debug(f"[LLM RECORDER RESPONSE] Raw response snippet:\n{response[:500]}...")

        suggestion_json = self._clean_llm_response_to_json(response)

        if not suggestion_json:
            logger.error("[LLM Suggestion Failed] Failed to get a valid JSON suggestion from LLM.")
            self._add_to_history("LLM Suggestion Failed", {"raw_response_snippet": response[:500]+"..."})
            return None # Indicate failure to suggest

        action = suggestion_json.get("action")
        reasoning = suggestion_json.get("reasoning", "No reasoning provided.")
        logger.info(f"[LLM Suggestion] Action: {action}, Params: {suggestion_json.get('parameters')}, Reasoning: {reasoning[:150]}...")
        self._add_to_history("LLM Suggestion", suggestion_json)

        # --- Process Suggestion ---
        if action in ["click", "type"]:
            target_index = suggestion_json.get("parameters", {}).get("index")
            if target_index is None or not isinstance(target_index, int) or target_index < 0:
                 logger.error(f"LLM suggested action '{action}' but provided invalid index: {target_index}")
                 return {"action": "suggestion_failed", "parameters": {}, "reasoning": f"LLM provided invalid index '{target_index}' for action '{action}'."}

            # --- Retrieve the node and pre-generated selector ---
            if self._latest_dom_state is None or not self._latest_dom_state.selector_map:
                 logger.error("DOM state or selector map is missing, cannot lookup suggested index.")
                 return {"action": "suggestion_failed", "parameters": {}, "reasoning": "Internal error: DOM state unavailable."}

            target_node = self._latest_dom_state.selector_map.get(target_index)
            if target_node is None:
                 available_indices = list(self._latest_dom_state.selector_map.keys())
                 logger.error(f"LLM suggested index [{target_index}], but it was not found in DOM context map. Available: {available_indices}")
                 return {"action": "suggestion_failed", "parameters": {}, "reasoning": f"Suggested element index [{target_index}] not found in current page context."}

            suggested_selector = target_node.css_selector
            if not suggested_selector:
                 # Try to generate it now if missing (should have been done in get_structured_dom)
                 suggested_selector = self.browser_controller.get_selector_for_node(target_node)
                 if suggested_selector:
                      target_node.css_selector = suggested_selector # Cache it
                 else:
                      logger.error(f"Could not generate selector for suggested index [{target_index}] (Node: {target_node.tag_name}).")
                      return {"action": "suggestion_failed", "parameters": {}, "reasoning": f"Failed to generate CSS selector for suggested index [{target_index}]."}

            logger.info(f"LLM suggested index [{target_index}], resolved to selector: '{suggested_selector}'")
            return {
                "action": action,
                "parameters": suggestion_json.get("parameters", {}), # Pass along original params (like text)
                "suggested_selector": suggested_selector,
                "target_node": target_node, # Pass the node for potential use
                "reasoning": reasoning
            }
        elif action in ["action_not_applicable", "suggestion_failed"]:
             # Pass these through directly
             return suggestion_json
        else:
            logger.error(f"LLM returned unknown action type: {action}")
            return {"action": "suggestion_failed", "parameters": {}, "reasoning": f"LLM returned unknown action '{action}'."}


    def _execute_action_for_recording(self, action: str, selector: Optional[str], parameters: Dict[str, Any]) -> Dict[str, Any]:
        """
        Executes a specific browser action (navigate, click, type) during recording.
        This is called *after* user confirmation/override. It does not involve AI decision.
        """
        result = {"success": False, "message": f"Action '{action}' invalid.", "data": None}

        if not action:
            result["message"] = "No action specified for execution."
            logger.warning(f"[RECORDER_EXEC] {result['message']}")
            return result

        logger.info(f"[RECORDER_EXEC] Executing: {action} | Selector: {selector} | Params: {parameters}")
        self._add_to_history("Executing Recorder Action", {"action": action, "selector": selector, "parameters": parameters})

        try:
            if action == "navigate":
                url = parameters.get("url")
                if not url or not isinstance(url, str): raise ValueError("Missing or invalid 'url'.")
                self.browser_controller.goto(url)
                result["success"] = True
                result["message"] = f"Navigated to {url}."
                # Add implicit wait for load state after navigation
                self.recorded_steps.append({
                    "step_id": self._current_step_id, # Use internal counter
                    "action": "wait_for_load_state",
                    "description": "Wait for page navigation to complete",
                    "parameters": {"state": "domcontentloaded"}, # Reasonable default
                    "selector": None,
                    "wait_after_secs": 0
                })
                self._current_step_id += 1 # Increment after adding implicit step

            elif action == "click":
                if not selector: raise ValueError("Missing selector for click action.")
                self.browser_controller.click(selector)
                result["success"] = True
                result["message"] = f"Clicked element: {selector}."

            elif action == "type":
                text = parameters.get("text")
                if not selector: raise ValueError("Missing selector for type action.")
                if text is None: raise ValueError("Missing or invalid 'text'.") # Allow empty string? yes.
                self.browser_controller.type(selector, text)
                result["success"] = True
                result["message"] = f"Typed into element: {selector}."

            elif action == "scroll": # Basic scroll support if planned
                direction = parameters.get("direction")
                if direction not in ["up", "down"]: raise ValueError("Invalid scroll direction.")
                self.browser_controller.scroll(direction)
                result["success"] = True
                result["message"] = f"Scrolled {direction}."

            else:
                result["message"] = f"Action '{action}' is not directly executable during recording via this method."
                logger.warning(f"[RECORDER_EXEC] {result['message']}")


        except (PlaywrightError, PlaywrightTimeoutError, ValueError) as e:
            error_msg = f"Execution during recording failed for action '{action}' on selector '{selector}': {type(e).__name__}: {e}"
            logger.error(f"[RECORDER_EXEC] {error_msg}", exc_info=False)
            result["message"] = error_msg
            result["success"] = False
             # Optionally save screenshot on execution failure *during recording*
            try:
                ts = time.strftime("%Y%m%d_%H%M%S")
                fname = f"output/recorder_exec_fail_{action}_{ts}.png"
                self.browser_controller.save_screenshot(fname)
                logger.info(f"Saved screenshot on recorder execution failure: {fname}")
            except: pass # Ignore screenshot errors here

        except Exception as e:
            error_msg = f"Unexpected Error during recorder execution action '{action}': {type(e).__name__}: {e}"
            logger.critical(f"[RECORDER_EXEC] {error_msg}", exc_info=True)
            result["message"] = error_msg
            result["success"] = False

        # Log Action Result
        log_level = logging.INFO if result["success"] else logging.WARNING
        logger.log(log_level, f"[RECORDER_EXEC_RESULT] Action '{action}' | Success: {result['success']} | Message: {result['message']}")
        self._add_to_history("Recorder Action Result", {"success": result["success"], "message": result["message"]})

        return result

    # --- New Recorder Core Logic ---

    def _handle_interactive_step_recording(self, planned_step: Dict[str, Any], suggestion: Dict[str, Any]) -> bool:
        """
        Handles the user interaction loop for a suggested 'click' or 'type' action.
        Returns True if the step was successfully recorded (or skipped), False if aborted.
        """
        action = suggestion["action"]
        suggested_selector = suggestion["suggested_selector"]
        target_node = suggestion["target_node"] # DOMElementNode
        parameters = suggestion["parameters"] # Contains index and potentially text
        reasoning = suggestion.get("reasoning", "N/A")
        planned_desc = planned_step["description"]

        final_selector = None
        performed_action = False

        print("\n" + "="*60)
        print(f"Planned Step: {planned_desc}")
        print(f"AI Suggestion: Action='{action}', Target='{target_node.tag_name}' (Reason: {reasoning})")
        print(f"Suggested Selector: {suggested_selector}")
        print("="*60)

        # Highlight suggested element
        self.browser_controller.clear_highlights()
        self.browser_controller.highlight_element(suggested_selector, target_node.highlight_index, color="#FFA500", text="AI Suggestion") # Orange for suggestion

        # Setup listener *before* prompt
        listener_setup = self.browser_controller.setup_click_listener()
        if not listener_setup:
             logger.error("Failed to set up click listener, cannot proceed with override.")
             # Optionally fallback to just accepting AI suggestion? Or abort? Let's abort for safety.
             return False # Indicate failure/abort


        override_selector = None

        # Use a separate thread for input to allow click detection simultaneously
        # Note: This basic input approach might block other activities if not handled carefully.
        # Consider more robust async/event-driven approaches for complex GUIs.
        try:
            logger.debug("Calling wait_for_user_click_or_timeout (wait_for_function version)...")
            # Call the updated wait function - it now returns the selector string or None
            override_selector = self.browser_controller.wait_for_user_click_or_timeout(INTERACTIVE_TIMEOUT_SECS)
            logger.debug(f"Returned from wait_for_user_click_or_timeout. override_selector = {override_selector}")

            # --- Handle Override First ---
            if override_selector is not None: # Check if a selector string was returned
                print(f"\n[Recorder] User override detected! Using selector: {override_selector}")
                final_selector = override_selector
                performed_action = False

                # Execute the original *intended* action on the *overridden* selector
                print(f"Executing original action '{action}' on overridden selector...")
                exec_result = self._execute_action_for_recording(action, final_selector, parameters)
                performed_action = exec_result["success"]

                if performed_action:
                    # --- Record the successful override step ---
                    record = {
                        "step_id": self._current_step_id,
                        "action": action,
                        "description": planned_desc,
                        "parameters": {},
                        "selector": final_selector,
                        "wait_after_secs": DEFAULT_WAIT_AFTER_ACTION
                    }
                    if action == "type":
                        record["parameters"]["text"] = parameters.get("text", "")
                        param_name_input = input(f"  Parameterize value '{parameters.get('text')}'? Enter name or leave blank: ").strip()
                        if param_name_input: record["parameters"]["parameter_name"] = param_name_input
                    self.recorded_steps.append(record)
                    self._current_step_id += 1
                    logger.info(f"Step {record['step_id']} recorded (User Override): {action} on {final_selector}")
                    self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "done", result=f"Recorded override as step {record['step_id']}")
                    return True # <<<--- RETURN HERE: Override successful bypass console prompt

                else: # Override execution failed
                    print(f"WARNING: Execution failed using override selector: {exec_result['message']}")
                    retry_choice = input("Override execution failed. Skip (S) or Abort (A)? > ").strip().lower()
                    if retry_choice == 'a':
                        self._user_abort_recording = True
                        return False # Abort
                    else:
                        print("Skipping step after failed override execution.")
                        self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Skipped after failed override execution")
                        return True # Skip, RETURN HERE Bypass console prompt
            else:
                logger.debug("No click override registered (wait_for_function timed out), prompting user via console.")
                prompt = (
                    f"AI suggests '{action}' on the highlighted element with selector `{suggested_selector}`.\n"
                    f"  Accept suggestion? (Press Enter or Y)\n"
                    f"  Skip this step? (S)\n"
                    f"  Abort recording? (A)\n"
                    f"Your choice? > "
                )
                user_choice = input(prompt).strip().lower()

                # --- Process Console User Choice ---
                if user_choice == '' or user_choice == 'y':
                    print("Accepting AI suggestion.")
                    final_selector = suggested_selector
                    performed_action = False # Initialize

                    # Execute action on AI's suggested selector
                    exec_result = self._execute_action_for_recording(action, final_selector, parameters)
                    performed_action = exec_result["success"]

                    if performed_action:
                         # --- Record the successful AI suggestion step ---
                         record = {
                            "step_id": self._current_step_id,
                            "action": action,
                            "description": planned_desc, # Use original description
                            "parameters": {},
                            "selector": final_selector,
                            "wait_after_secs": DEFAULT_WAIT_AFTER_ACTION
                         }
                         if action == "type":
                             record["parameters"]["text"] = parameters.get("text", "")
                             # Ask for parameterization
                             param_name_input = input(f"  Parameterize value '{parameters.get('text')}'? Enter name or leave blank: ").strip()
                             if param_name_input:
                                 record["parameters"]["parameter_name"] = param_name_input

                         self.recorded_steps.append(record)
                         self._current_step_id += 1
                         logger.info(f"Step {record['step_id']} recorded (AI Suggestion): {action} on {final_selector}")
                         self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "done", result=f"Recorded AI suggestion as step {record['step_id']}")
                         return True # Success

                    else: # AI suggestion execution failed
                         print(f"WARNING: Execution failed using AI suggested selector: {exec_result['message']}")
                         retry_choice = input("Execution failed. Retry suggestion (R), Skip (S) or Abort (A)? > ").strip().lower()
                         if retry_choice == 'a':
                             self._user_abort_recording = True
                             return False # Abort
                         elif retry_choice == 'r':
                              print("Marking step for retry...")
                              current_task_index = self.task_manager.current_subtask_index
                              self.task_manager.update_subtask_status(current_task_index, "failed", error="User requested retry after execution failure.")
                              return True # Signal to loop to retry planning/suggestion
                         else:
                              print("Skipping step after failed execution.")
                              self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Skipped after failed AI suggestion execution")
                              return True # Skip

                elif user_choice == 's':
                    print("Skipping planned step.")
                    self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="User skipped")
                    return True # Skip

                elif user_choice == 'a':
                    print("Aborting recording process.")
                    self._user_abort_recording = True
                    return False # Abort

                else:
                    print("Invalid choice. Skipping step.")
                    self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Invalid user choice")
                    return True # Skip

        except Exception as e:
             logger.error(f"Error during user interaction: {e}", exc_info=True)
             user_choice = 'a' # Abort on error

        # --- Process User Choice ---
        # Now use override_selector which was set if final_clicked_selector was found
        if user_choice == 'override' or override_selector:
            print(f"Using override selector: {override_selector}")
            final_selector = override_selector
            # Execute the original *intended* action on the *overridden* selector
            print(f"Executing original action '{action}' on overridden selector...")
            exec_result = self._execute_action_for_recording(action, final_selector, parameters)
            performed_action = exec_result["success"]
            if not performed_action:
                 print(f"WARNING: Execution failed using override selector: {exec_result['message']}")
                 retry_choice = input("Execution failed. Skip (S) or Abort (A)? > ").strip().lower()
                 if retry_choice == 'a':
                      self._user_abort_recording = True
                      return False # Abort
                 else:
                      print("Skipping step after failed override execution.")
                      self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Skipped after failed override execution")
                      return True # Skip


        elif user_choice == '' or user_choice == 'y':
            print("Accepting AI suggestion.")
            final_selector = suggested_selector
            # Execute action on AI's suggested selector
            exec_result = self._execute_action_for_recording(action, final_selector, parameters)
            performed_action = exec_result["success"]
            if not performed_action:
                 print(f"WARNING: Execution failed using AI suggested selector: {exec_result['message']}")
                 retry_choice = input("Execution failed. Retry suggestion (R), Skip (S) or Abort (A)? > ").strip().lower()
                 if retry_choice == 'a':
                     self._user_abort_recording = True
                     return False # Abort
                 elif retry_choice == 'r':
                      print("Marking step for retry...")
                      current_task_index = self.task_manager.current_subtask_index
                      self.task_manager.update_subtask_status(current_task_index, "failed", error="User requested retry after execution failure.")
                      # Important: Ensure no step is recorded when retrying
                      return True # Signal to loop to retry planning/suggestion
                 else:
                      print("Skipping step after failed execution.")
                      self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Skipped after failed AI suggestion execution")
                      return True # Skip
            else:
                self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "done", result="Executed AI suggestion")

        elif user_choice == 's':
            print("Skipping planned step.")
            # Mark task manager step as skipped? Or just don't record. Let's not record.
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="User skipped")
            performed_action = False # No action performed or recorded
            return True # Continue to next planned step

        elif user_choice == 'a':
            print("Aborting recording process.")
            self._user_abort_recording = True
            return False # Signal abort to main loop

        else:
            print("Invalid choice. Skipping step.")
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Invalid user choice")
            performed_action = False
            return True # Continue to next planned step

        # --- Record Step if Action Performed ---
        if performed_action and final_selector:
            record = {
                "step_id": self._current_step_id,
                "action": action,
                "description": planned_desc, # Use original planned description
                "parameters": {}, # Initialize empty params
                "selector": final_selector,
                "wait_after_secs": DEFAULT_WAIT_AFTER_ACTION # Add small default wait
            }
            if action == "type":
                record["parameters"]["text"] = parameters.get("text", "") # Get text from original params
                # Add parameterization hook (simple example)
                param_name_input = input(f"  Parameterize value '{parameters.get('text')}'? Enter name (e.g., username) or leave blank: ").strip()
                if param_name_input:
                     record["parameters"]["parameter_name"] = param_name_input

            self.recorded_steps.append(record)
            self._current_step_id += 1
            logger.info(f"Step {record['step_id']} recorded: {action} on {final_selector}")
            return True # Success
        elif final_selector: # Action wasn't performed (e.g., failed override) but we decided not to abort/skip
            # This case should be handled by the return False or True for skip/abort above
             logger.warning("Reached unexpected state where action wasn't performed but step wasn't skipped/aborted.")
             return True # Treat as skipped
        else: # No selector finalized (e.g., abort/skip)
            return True # Already handled by returns above


    def _handle_assertion_recording(self, planned_step: Dict[str, Any]) -> bool:
        """
        Handles prompting the user for assertion details based on a 'Verify...' planned step.
        Returns True if recorded/skipped, False if aborted.
        """
        planned_desc = planned_step["description"]
        print("\n" + "="*60)
        print(f"Planned Step: {planned_desc}")
        print("This is a verification step. Let's define the assertion.")
        print("="*60)

        # 1. Identify Target Element (Use LLM again, simplified prompt)
        #    We need a selector for the element to assert against.
        current_url = self.browser_controller.get_current_url()
        dom_context_str = "Error getting DOM"
        if self._latest_dom_state:
            dom_context_str = self._latest_dom_state.element_tree.generate_llm_context_string()

        prompt = f"""
        Given the verification step: "{planned_desc}"
        And the current visible interactive elements:
        ```html
        {dom_context_str}
        ```
        Identify the element index `[index]` most relevant to this verification.
        Output JSON: {{"index": INDEX_NUMBER}} or {{"index": null, "reasoning": "Cannot determine target element"}}
        ```json
        """
        logger.debug(f"[LLM ASSERT PROMPT] Sending prompt for assertion target index:\n{prompt[:500]}...")
        response = self.gemini_client.generate_text(prompt)
        logger.debug(f"[LLM ASSERT RESPONSE] Raw response snippet:\n{response[:500]}...")
        target_json = self._clean_llm_response_to_json(response)

        target_node = None
        target_selector = None

        if target_json and target_json.get("index") is not None:
            target_index = target_json.get("index")
            if self._latest_dom_state and self._latest_dom_state.selector_map:
                 target_node = self._latest_dom_state.selector_map.get(target_index)
                 if target_node and target_node.css_selector:
                      target_selector = target_node.css_selector
                      print(f"AI suggests asserting on element [Index: {target_index}]: <{target_node.tag_name}> with selector: `{target_selector}`")
                      self.browser_controller.clear_highlights()
                      self.browser_controller.highlight_element(target_selector, target_index, color="#0000FF", text="Assert Target?") # Blue for assert
                 else:
                      print("AI suggested an index, but element or selector not found in current context.")
            else:
                 print("AI suggested an index, but DOM context is unavailable.")
        else:
            reason = "LLM response parsing failed." # Default reason
            if target_json: # If JSON was parsed but index was null/missing
                reason = target_json.get('reasoning', 'LLM did not provide a target index.')
            elif response: # If JSON parsing failed but we have raw response
                 reason = f"LLM response parsing failed. Raw response snippet: {response[:100]}..."
            print(f"AI could not confidently identify a target element for '{planned_desc}'. Reason: {reason}")

        # --- User confirms/overrides target selector ---
        override_prompt = "Enter a different selector if needed, or press Enter to use suggested/skip, or A to abort: > "
        user_input_selector = input(override_prompt).strip()

        if user_input_selector.lower() == 'a':
             self._user_abort_recording = True
             return False
        elif user_input_selector == '' and target_selector:
             final_selector = target_selector
             print(f"Using suggested selector: {final_selector}")
        elif user_input_selector != '':
             final_selector = user_input_selector
             print(f"Using user-provided selector: {final_selector}")
             # Optionally try to highlight the user's selector
             try:
                  self.browser_controller.clear_highlights()
                  self.browser_controller.highlight_element(final_selector, 0, color="#00FF00", text="User Target") # Green for user choice
             except Exception as e:
                  print(f"Warning: Could not highlight user selector '{final_selector}': {e}")
        else: # User hit Enter without suggestion or providing one
            print("No target selector specified. Skipping assertion.")
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="User skipped assertion target")
            return True # Skip

        # --- User Selects Assertion Type ---
        print("\nChoose Assertion Type:")
        print("  [1] Text Contains")
        print("  [2] Text Equals")
        print("  [3] Is Visible")
        print("  [4] Is Hidden (Not Visible)")
        print("  [5] Attribute Equals")
        print("  [6] Element Count Equals")
        print("  [S] Skip Assertion")
        print("  [A] Abort Recording")
        assert_choice = input("Enter choice: > ").strip().lower()

        assertion_action = None
        assertion_params = {}

        if assert_choice == '1':
            assertion_action = "assert_text_contains"
            expected_text = input("Enter expected text to contain: ").strip()
            assertion_params["expected_text"] = expected_text
        elif assert_choice == '2':
            assertion_action = "assert_text_equals"
            expected_text = input("Enter exact expected text: ").strip()
            assertion_params["expected_text"] = expected_text
        elif assert_choice == '3':
            assertion_action = "assert_visible"
        elif assert_choice == '4':
            assertion_action = "assert_hidden"
        elif assert_choice == '5':
            assertion_action = "assert_attribute_equals"
            attr_name = input("Enter attribute name (e.g., 'class', 'disabled'): ").strip()
            expected_value = input(f"Enter expected value for attribute '{attr_name}': ").strip()
            assertion_params["attribute_name"] = attr_name
            assertion_params["expected_value"] = expected_value
        elif assert_choice == '6':
             assertion_action = "assert_element_count"
             expected_count = input("Enter expected element count: ").strip()
             try:
                 assertion_params["expected_count"] = int(expected_count)
             except ValueError:
                 print("Invalid count. Skipping assertion.")
                 self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Invalid assertion count")
                 return True # Skip
        elif assert_choice == 's':
            print("Skipping assertion.")
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="User skipped assertion")
            return True # Skip
        elif assert_choice == 'a':
            print("Aborting recording.")
            self._user_abort_recording = True
            return False # Abort
        else:
            print("Invalid assertion choice. Skipping assertion.")
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Invalid assertion choice")
            return True # Skip

        # --- Record the Assertion Step ---
        if assertion_action and final_selector:
            record = {
                "step_id": self._current_step_id,
                "action": assertion_action,
                "description": planned_desc, # Use original planned description
                "parameters": assertion_params,
                "selector": final_selector,
                "wait_after_secs": 0 # Assertions usually don't need waits after
            }
            self.recorded_steps.append(record)
            self._current_step_id += 1
            logger.info(f"Step {record['step_id']} recorded: {assertion_action} on {final_selector}")
             # Mark original planned step as done in task manager
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "done", result=f"Recorded as assertion step {record['step_id']}")
            return True
        else:
            # Should have been handled by skip/abort logic above
            logger.warning("Reached end of assertion handling without recording, skipping.")
            self.task_manager.update_subtask_status(self.task_manager.current_subtask_index, "skipped", result="Assertion not fully defined")
            return True # Skip


    def record(self, feature_description: str) -> Dict[str, Any]:
        """
        Runs the interactive test recording process.
        """
        if not self.is_recorder_mode:
             logger.error("Cannot run record() method when not in recorder mode.")
             return {"success": False, "message": "Agent not initialized in recorder mode."}

        logger.info(f"--- Starting Test Recording --- Feature: {feature_description}")
        print(f"\n--- Starting Recording for Feature ---\n{feature_description}\n" + "-"*35)
        start_time = time.time()
        # Initialize recording status
        recording_status = {
            "success": False,
            "feature": feature_description,
            "message": "Recording initiated.",
            "output_file": None,
            "steps_recorded": 0,
            "duration_seconds": 0.0,
        }
        self.history = []
        self.recorded_steps = []
        self._current_step_id = 1
        self.output_file_path = None
        self._latest_dom_state = None
        self._user_abort_recording = False

        try:
            logger.debug("[RECORDER] Starting browser controller...")
            self.browser_controller.start()
            self.browser_controller.clear_console_messages()

            self.task_manager.set_main_task(feature_description)
            logger.debug("[RECORDER] Planning initial steps...")
            self._plan_subtasks(feature_description) # Generates the list of planned steps

            if not self.task_manager.subtasks:
                 recording_status["message"] = "❌ Recording Planning Failed: No steps generated."
                 raise Exception(recording_status["message"])

            planned_steps_count = len(self.task_manager.subtasks)
            logger.info(f"Beginning interactive recording for {planned_steps_count} planned steps...")
            iteration_count = 0

            while iteration_count < self.max_iterations: # Use max_iterations as limit on planned steps
                iteration_count += 1
                current_planned_task = self.task_manager.get_next_subtask()

                if self._user_abort_recording:
                     recording_status["message"] = "Recording aborted by user."
                     logger.warning(recording_status["message"])
                     break # Exit loop if user aborted

                if not current_planned_task:
                    # Check if finished normally or failed planning/retries
                    if self.task_manager.is_complete():
                         all_done_or_skipped = all(t['status'] in ['done', 'skipped'] for t in self.task_manager.subtasks)
                         if all_done_or_skipped:
                              logger.info("All planned steps processed or skipped.")
                              recording_status["message"] = "Recording process completed."
                              recording_status["success"] = True # Mark as success if finished cleanly
                         else:
                              # Find first non-done/skipped task
                              failed_task = next((t for t in self.task_manager.subtasks if t['status'] not in ['done', 'skipped']), None)
                              if failed_task:
                                   recording_status["message"] = f"Recording stopped. Failed to process planned step: {failed_task['description']} (Status: {failed_task['status']})."
                                   logger.error(recording_status["message"])
                              else: # Should not happen
                                   recording_status["message"] = "Recording finished, but final state unclear."
                                   logger.warning(recording_status["message"])
                    else:
                         recording_status["message"] = "Recording loop ended unexpectedly."
                         logger.error(recording_status["message"])
                    break # Exit loop

                current_task_index = self.task_manager.current_subtask_index # Index from task manager
                logger.info(f"\n===== Processing Planned Step {current_task_index + 1}/{planned_steps_count} (Attempt {current_planned_task['attempts']}) =====")
                print(f"\nProcessing Planned Step {current_task_index + 1}: {current_planned_task['description']}")

                # --- State Gathering ---
                logger.info("Gathering browser state and structured DOM for suggestion...")
                current_url = "Error: Could not get URL"
                dom_context_str = "Error: Could not process DOM"
                self._latest_dom_state = None
                self.browser_controller.clear_highlights() # Clear previous highlights

                try:
                    current_url = self.browser_controller.get_current_url()
                    if not current_url.startswith("Error"):
                        # Get structured DOM WITH pre-generated selectors, NO JS highlighting
                        self._latest_dom_state = self.browser_controller.get_structured_dom(highlight_all_clickable_elements=False)
                        if self._latest_dom_state and self._latest_dom_state.element_tree:
                            dom_context_str = self._latest_dom_state.element_tree.generate_llm_context_string()
                            # logger.debug(f"[RECORDER] DOM Context for LLM (length: {len(dom_context_str)}):\n{dom_context_str[:1000]}...") # Debug log
                        else:
                            dom_context_str = "Error processing DOM structure."
                            logger.error("[RECORDER] Failed to get valid DOM state.")
                    else:
                        dom_context_str = "Error: Could not get current URL."
                except Exception as e:
                    logger.error(f"Failed to gather browser state/DOM: {e}", exc_info=True)
                    dom_context_str = f"Error gathering state: {e}"
                    current_url = "Error gathering state"
                    # Allow proceeding, maybe LLM can handle navigation step

                # --- Handle Step Type ---
                planned_step_desc = current_planned_task['description'].lower()
                step_handled = False

                if planned_step_desc.startswith("navigate to"):
                    try:
                        # --- MODIFICATION START ---
                        # Use re.split for case-insensitive split and check length
                        parts = re.split("navigate to", current_planned_task['description'], maxsplit=1, flags=re.IGNORECASE)
                        if len(parts) > 1:
                            url = parts[1].strip()
                            if url:
                                print(f"Action: Navigate to {url}")
                                # Execute directly
                                exec_result = self._execute_action_for_recording("navigate", None, {"url": url})
                                if exec_result["success"]:
                                    # Record navigation step (keep existing logic)
                                    self.recorded_steps.append({
                                        "step_id": self._current_step_id,
                                        "action": "navigate",
                                        "description": current_planned_task['description'],
                                        "parameters": {"url": url},
                                        "selector": None,
                                        "wait_after_secs": DEFAULT_WAIT_AFTER_ACTION
                                    })
                                    self._current_step_id += 1
                                    logger.info(f"Step {self._current_step_id-1} recorded: navigate to {url}")
                                    self.task_manager.update_subtask_status(current_task_index, "done", result="Recorded navigation")
                                else:
                                    print(f"WARNING: Navigation failed: {exec_result['message']}")
                                    choice = input("Navigation failed. Skip (S) or Abort (A)? > ").strip().lower()
                                    if choice == 'a': self._user_abort_recording = True
                                    else: self.task_manager.update_subtask_status(current_task_index, "skipped", result="Skipped failed navigation")
                            else:
                                # This case means "navigate to" was at the very end
                                raise ValueError("Could not parse URL after 'navigate to'.")
                        else:
                             # This means "navigate to" wasn't found case-insensitively
                             raise ValueError("Could not find 'navigate to' phrase to parse URL.")
                        # --- MODIFICATION END ---
                        step_handled = True
                    except Exception as nav_e:
                         logger.error(f"Error handling navigation step '{current_planned_task['description']}': {nav_e}")
                         self.task_manager.update_subtask_status(current_task_index, "failed", error=f"Failed to parse/execute navigation: {nav_e}")
                         step_handled = True # Handled as failure


                elif planned_step_desc.startswith("verify") or planned_step_desc.startswith("check") or planned_step_desc.startswith("assert"):
                    if not self._handle_assertion_recording(current_planned_task):
                         # False means user aborted
                         self._user_abort_recording = True
                    step_handled = True # Assertion handled (recorded, skipped, or aborted)

                elif planned_step_desc.startswith("scroll"):
                     try:
                          direction = "down" if "down" in planned_step_desc else "up" if "up" in planned_step_desc else None
                          if direction:
                               print(f"Action: Scroll {direction}")
                               exec_result = self._execute_action_for_recording("scroll", None, {"direction": direction})
                               if exec_result["success"]:
                                    self.recorded_steps.append({
                                         "step_id": self._current_step_id,
                                         "action": "scroll",
                                         "description": current_planned_task['description'],
                                         "parameters": {"direction": direction},
                                         "selector": None,
                                         "wait_after_secs": 0.2 # Short wait after scroll
                                    })
                                    self._current_step_id += 1
                                    logger.info(f"Step {self._current_step_id-1} recorded: scroll {direction}")
                                    self.task_manager.update_subtask_status(current_task_index, "done", result="Recorded scroll")
                               else:
                                     # Non-critical, just log and skip recording
                                     print(f"Optional scroll failed: {exec_result['message']}. Skipping recording.")
                                     self.task_manager.update_subtask_status(current_task_index, "skipped", result="Optional scroll failed")
                          else:
                               print(f"Could not determine scroll direction from: {current_planned_task['description']}. Skipping.")
                               self.task_manager.update_subtask_status(current_task_index, "skipped", result="Unknown scroll direction")
                          step_handled = True
                     except Exception as scroll_e:
                           logger.error(f"Error handling scroll step: {scroll_e}")
                           self.task_manager.update_subtask_status(current_task_index, "failed", error=f"Scroll step failed: {scroll_e}")
                           step_handled = True # Handled as failure

                # --- Default: Assume Interactive Click/Type ---
                if not step_handled:
                    # --- AI Suggestion ---
                    ai_suggestion = self._determine_action_and_selector_for_recording(
                        current_planned_task, current_url, dom_context_str
                    )

                    if not ai_suggestion or ai_suggestion.get("action") == "suggestion_failed":
                        reason = ai_suggestion.get("reasoning", "LLM failed to provide valid suggestion.") if ai_suggestion else "LLM suggestion failed."
                        logger.error(f"AI failed to suggest action/selector for step {current_task_index + 1}: {reason}")
                        self.task_manager.update_subtask_status(current_task_index, "failed", error=reason)
                        # Don't break loop immediately, allow retry based on TaskManager logic
                        continue # Continue to next iteration, TaskManager handles retries

                    elif ai_suggestion.get("action") == "action_not_applicable":
                         reason = ai_suggestion.get("reasoning", "Step not a click/type.")
                         logger.info(f"Planned step '{current_planned_task['description']}' determined as not applicable for interactive recording by AI. Skipping. Reason: {reason}")
                         self.task_manager.update_subtask_status(current_task_index, "skipped", result=f"Skipped non-interactive step ({reason})")
                         # Continue to next planned step

                    elif ai_suggestion.get("action") in ["click", "type"]:
                        # --- Handle Interactive Step ---
                        if not self._handle_interactive_step_recording(current_planned_task, ai_suggestion):
                            # False means user aborted
                             self._user_abort_recording = True
                        # If True, step was recorded, skipped, or retry requested. TaskManager status updated internally.

                    else: # Should not happen if _determine... is correct
                        logger.error(f"Unexpected AI suggestion action: {ai_suggestion.get('action')}. Skipping step.")
                        self.task_manager.update_subtask_status(current_task_index, "failed", error="Unexpected AI action suggestion")


                # --- Cleanup after processing a step ---
                self.browser_controller.clear_highlights()
                self.browser_controller.remove_click_listener() # Ensure listener is off

                # Small delay between steps
                time.sleep(0.5)


            # --- Loop End ---
            if not recording_status["success"] and iteration_count >= self.max_iterations:
                 recording_status["message"] = f"⚠️ Recording Stopped: Maximum planned steps ({self.max_iterations}) processed."
                 logger.warning(recording_status["message"])

            # --- Final Save ---
            if not self._user_abort_recording and self.recorded_steps:
                try:
                    output_data = {
                        "test_name": f"{feature_description[:50]}_Test",
                        "feature_description": feature_description,
                        "recorded_at": datetime.utcnow().isoformat() + "Z",
                        "steps": self.recorded_steps
                    }
                    ts = time.strftime("%Y%m%d_%H%M%S")
                    safe_feature_name = re.sub(r'[^\w\-]+', '_', feature_description)[:50]
                    filename = f"test_{safe_feature_name}_{ts}.json"
                    self.output_file_path = os.path.join("output", filename)

                    with open(self.output_file_path, 'w', encoding='utf-8') as f:
                        json.dump(output_data, f, indent=2, ensure_ascii=False)

                    recording_status["output_file"] = self.output_file_path
                    recording_status["steps_recorded"] = len(self.recorded_steps)
                    # Set success only if we saved something and didn't explicitly fail/abort
                    if recording_status.get("message") == "Recording process completed.": # Check if completion message was set
                         recording_status["success"] = True
                         logger.info(f"Recording successfully saved to: {self.output_file_path}")
                    else: # Finished but maybe with errors or max iterations
                         logger.warning(f"Recording finished but status indicates potential issues. Saved {len(self.recorded_steps)} steps to: {self.output_file_path}")
                         # Keep success as False unless explicitly completed okay


                except Exception as save_e:
                    logger.error(f"Failed to save recorded steps to JSON: {save_e}", exc_info=True)
                    recording_status["message"] = f"Failed to save recording: {save_e}"
                    recording_status["success"] = False
            elif self._user_abort_recording:
                 recording_status["message"] = "Recording aborted by user. No file saved."
                 recording_status["success"] = False
            else: # No steps recorded
                 recording_status["message"] = "No steps were recorded."
                 recording_status["success"] = False


        except ValueError as e: # Catch planning errors specifically
             logger.critical(f"Test planning failed: {e}", exc_info=True)
             recording_status["message"] = f"❌ Test Planning Failed: {e}"
        except Exception as e:
            logger.critical(f"An critical unexpected error occurred during recording: {e}", exc_info=True)
            recording_status["message"] = f"❌ Critical Error during recording: {e}"
        finally:
            logger.info("--- Ending Test Recording ---")
            self.browser_controller.clear_highlights()
            self.browser_controller.remove_click_listener()
            if hasattr(self, 'browser_controller') and self.browser_controller:
                 self.browser_controller.close()

            end_time = time.time()
            recording_status["duration_seconds"] = round(end_time - start_time, 2)
            logger.info(f"Recording process finished in {recording_status['duration_seconds']:.2f} seconds.")
            logger.info(f"Final Recording Status: {'Success' if recording_status['success'] else 'Failed/Aborted'} - {recording_status['message']}")
            if recording_status["output_file"]:
                 logger.info(f"Output file: {recording_status['output_file']}")

        return recording_status # Return the detailed status dictionary

    # --- Legacy run method (can be removed or kept for non-recorder execution if needed) ---
    # def run(self, feature_description: str) -> Dict[str, Any]:
    #    if self.is_recorder_mode:
    #         logger.error("Cannot run legacy execution method in recorder mode. Use record() instead.")
    #         return {"status": "FAIL", "message": "Wrong mode"}
    #     # ... (Original run method logic would go here) ...
    #     logger.warning("Legacy run method executed.")
    #     # This part needs significant review if kept, as many helper methods were changed.
    #     # For now, assume it's deprecated by the Recorder/Executor model.
    #     return {"status": "FAIL", "message": "Legacy run method not fully supported anymore."}